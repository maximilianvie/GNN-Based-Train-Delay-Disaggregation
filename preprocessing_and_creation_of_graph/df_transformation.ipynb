{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def custom_timedelta(t):\n",
    "    try:\n",
    "        td = pd.to_timedelta(t)\n",
    "        return round(td.total_seconds(), 0)\n",
    "    except ValueError:\n",
    "        dot_count = t.count('.')\n",
    "        if dot_count == 1:\n",
    "            days, time = t.split('.')\n",
    "            try:\n",
    "                sign = -1 if days.startswith('-') else 1\n",
    "                days = abs(int(days))\n",
    "                hours, minutes, seconds = time.split(':')\n",
    "                total_seconds = sign * ((days * 24 * 3600) + (int(hours) * 3600) + (int(minutes) * 60) + round(float(seconds), 0))\n",
    "                return round(total_seconds, 0)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid time format, expected either hh:mm:ss or d.hh:mm:ss\")\n",
    "        elif dot_count == 2:\n",
    "            days_time, fraction = t.rsplit('.', 1)\n",
    "            days, time = days_time.split('.')\n",
    "            try:\n",
    "                sign = -1 if days.startswith('-') else 1\n",
    "                days = abs(int(days))\n",
    "                hours, minutes, seconds = time.split(':')\n",
    "                seconds = float(seconds) + round(float(f\"0.{fraction}\"), 0)\n",
    "                total_seconds = sign * ((days * 24 * 3600) + (int(hours) * 3600) + (int(minutes) * 60) + seconds)\n",
    "                return round(total_seconds, 0)\n",
    "            except ValueError:\n",
    "                raise ValueError(\"Invalid time format, expected either hh:mm:ss or d.hh:mm:ss\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid time format, expected either hh:mm:ss or d.hh:mm:ss\")\n",
    "\n",
    "# Initialize a list to store the metrics\n",
    "metrics_list = []\n",
    "\n",
    "def apply_transformations(file_path):\n",
    "\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert date columns to datetime, handling errors\n",
    "    datetime_cols = ['scheduled_arrival', 'scheduled_departure', 'arrival', 'departure']\n",
    "    for col in datetime_cols:\n",
    "        data[col] = pd.to_datetime(data[col], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "    ###################################################################\n",
    "    # Sorting data correctly. We cannot fully rely on sort_time, since sometimes the order of sequence_number \n",
    "    # does not match sort time. That's why it is necessary to sort as done below. \n",
    "    ###################################################################\n",
    "\n",
    "    # Create and sort 'sort_time' column\n",
    "    data['sort_time'] = data['arrival'].fillna(data['departure'])\n",
    "\n",
    "    # Group by 'train_number' and 'trainpart_id', and find the earliest 'sort_time' for each group.\n",
    "    earliest_sort_times = data.groupby(['train_number', 'trainpart_id'])['sort_time'].min().reset_index()\n",
    "\n",
    "    # Create a rank for each 'trainpart_id' within each 'train_number' based on the earliest 'sort_time'.\n",
    "    earliest_sort_times['trainpart_id_order'] = earliest_sort_times.groupby('train_number')['sort_time'].rank(method='first').astype(int)\n",
    "\n",
    "    # Merge this order back into the original dataframe.\n",
    "    data = data.merge(earliest_sort_times[['train_number', 'trainpart_id', 'trainpart_id_order']], on=['train_number', 'trainpart_id'], how='left')\n",
    "\n",
    "    # Now sort the dataframe by 'train_number', 'trainpart_id_order', and 'sequence_number'.\n",
    "    data.sort_values(by=['train_number', 'trainpart_id_order', 'sequence_number'], inplace=True)\n",
    "\n",
    "    # Generate the 'train_unique_sequence' \n",
    "    data['train_unique_sequence'] = data.groupby(['train_number']).cumcount() + 1\n",
    "\n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "\n",
    "    # Necessary - changing operator class to numeric and then to integers\n",
    "    data['operator_class'] = pd.to_numeric(data['operator_class'], errors='coerce')\n",
    "\n",
    "    # Fill NaN values if any resulted from coercion, then convert to integers\n",
    "    # Here, 0 is used as a placeholder, but you may choose a different value as appropriate\n",
    "    data['operator_class'] = data['operator_class'].fillna(0).astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    ####################################################################\n",
    "    # We need for all further steps only one traction unit, so we filter out any unnecessary traction units\n",
    "    ####################################################################\n",
    "\n",
    "\n",
    "    # Set the flag to True for each row initially\n",
    "    data['lowest_unique_sequence_flag'] = True\n",
    "\n",
    "    # Flag adjustment for multiple traction units\n",
    "    multiple_units = data['number_of_traction_units'] > 1 \n",
    "\n",
    "    group_min_sequence = data[multiple_units].groupby(['sort_time', 'db640_code', 'ocp_type', 'trainpart_id'])['train_unique_sequence'].transform('min')\n",
    "    data.loc[multiple_units, 'lowest_unique_sequence_flag'] = data[multiple_units]['train_unique_sequence'] == group_min_sequence\n",
    "    data = data[data['lowest_unique_sequence_flag']]\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    # Adjust arrival times for initial delays in freight trains\n",
    "    ###########################################################\n",
    "\n",
    "    data = adjust_arrival_for_initial_delays_in_freight(data)\n",
    "\n",
    "\n",
    "    #########################################################################################\n",
    "    # Map operator class and category to integers and export it to dictionaries for later use\n",
    "    #########################################################################################\n",
    "\n",
    "    data = map_columns_to_integers(data)\n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "    # Creating time features suitable for deep learning models\n",
    "    ##########################################################\n",
    "\n",
    "    data = create_sin_cos_time_features(data)\n",
    "\n",
    "    data = creating_continuous_day_feature(data)\n",
    "\n",
    "    #################\n",
    "\n",
    "\n",
    "    # Convert delay times to timedelta and seconds\n",
    "\n",
    "    data['departure_delay_seconds'] = data['departure_delay_in_seconds'].apply(custom_timedelta)\n",
    "\n",
    "    data['arrival_delay_seconds'] = data['arrival_delay_in_seconds'].apply(custom_timedelta)\n",
    "\n",
    "    data['primary_delay'] = data['primary_delay'].apply(custom_timedelta)\n",
    "\n",
    "\n",
    "\n",
    "    # Sort and reset index\n",
    "    data.sort_values(by=['train_number', 'train_unique_sequence'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    \n",
    "    columns_to_keep = [\n",
    "    'operator_class', \n",
    "    'train_number', \n",
    "    'ocp_type',\n",
    "    'arrival', \n",
    "    'departure', \n",
    "    'train_unique_sequence',\n",
    "    'number_of_traction_units',  \n",
    "    'freight', \n",
    "    'trainpart_weight', \n",
    "    'longitude', \n",
    "    'latitude',\n",
    "    'category',\n",
    "    'trainpart_id',\n",
    "    'departure_delay_seconds', \n",
    "    'arrival_delay_seconds',\n",
    "    'sequence_number',\n",
    "    'db640_code',\n",
    "    'sin_seconds_since_midnight_departure',\n",
    "    'cos_seconds_since_midnight_departure',\n",
    "    'sin_seconds_since_midnight_arrival',\n",
    "    'cos_seconds_since_midnight_arrival',\n",
    "    'day_value',\n",
    "    'primary_delay',\n",
    "    'task_id'\n",
    "    ]\n",
    "\n",
    "    # Set negative delay_seconds to 0 - otherwise we also get delay jumps from negative to negative delay\n",
    "    data['departure_delay_seconds'] = data['departure_delay_seconds'].clip(lower=0)\n",
    "    data['arrival_delay_seconds'] = data['arrival_delay_seconds'].clip(lower=0)\n",
    "\n",
    "\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    data.sort_values(by=['train_number', 'train_unique_sequence'], inplace=True)\n",
    "\n",
    "    # Important - the delay jump departure is the delay increase, which is in the same rows as the PD \n",
    "    data['delay_jump'] = data.groupby('train_number')['departure_delay_seconds'].diff().fillna(data['departure_delay_seconds']).clip(lower=0)\n",
    "\n",
    "    data['secondary_delay'] =  (data['delay_jump'] - data['primary_delay']).clip(lower=0)\n",
    "    data['primary_delay_label'] = data['primary_delay'] >= data['secondary_delay']\n",
    "\n",
    "\n",
    "    # Step 4: Create label for delay jump above 60 seconds\n",
    "    data['nodes_to_classify'] = data['delay_jump'] > 60\n",
    "    \n",
    "\n",
    "    ############################################################################\n",
    "    # Compute statistics regarding class labels \n",
    "    ############################################################################\n",
    "\n",
    "    compute_statistics(data)\n",
    "\n",
    "    ############################################################################\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    # Forward fill NaN in longitude and latitude for the same train number\n",
    "    data['longitude'] = data.groupby('train_number')['longitude'].fillna(method='ffill')\n",
    "    data['latitude'] = data.groupby('train_number')['latitude'].fillna(method='ffill')\n",
    "\n",
    "    # Backward fill remaining NaN in longitude and latitude for the same train number\n",
    "    data['longitude'] = data.groupby('train_number')['longitude'].fillna(method='bfill')\n",
    "    data['latitude'] = data.groupby('train_number')['latitude'].fillna(method='bfill')\n",
    "    \n",
    "\n",
    "    # Check for duplicates in the 'scheduledID' column\n",
    "    if data['task_id'].duplicated().any():\n",
    "        print(\"Warning: 'scheduledID' does not uniquely identify each row.\")\n",
    "    else:\n",
    "        print(\"'task_id' uniquely identifies each row.\")\n",
    "\n",
    "    # 'sequence_number', 'trainpart_id', \n",
    "    missing_values = data.isnull().sum()\n",
    "\n",
    "    print(missing_values)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(data):\n",
    "    # Calculating metrics for overall data\n",
    "    total_nodes = len(data)\n",
    "    nodes_to_classify = data['nodes_to_classify'].sum()\n",
    "    percentage_nodes_to_classify = (nodes_to_classify / total_nodes) * 100\n",
    "    primary_delay_label_true = data[data['nodes_to_classify']]['primary_delay_label'].sum()\n",
    "    percentage_primary_delay_label_true = (primary_delay_label_true / nodes_to_classify) * 100\n",
    "    relevant_nodes = data[data['nodes_to_classify']]\n",
    "    instances_both_conditions = ((relevant_nodes['secondary_delay'] > 60) & (relevant_nodes['primary_delay'] > 60)).sum()\n",
    "    percentage_both_conditions = (instances_both_conditions / len(relevant_nodes)) * 100\n",
    "\n",
    "    print(\"Overall Metrics:\")\n",
    "    print(\"Percentage of nodes to classify: \", percentage_nodes_to_classify)\n",
    "    print(\"Percentage of nodes to classify where primary_delay_label is True: \", percentage_primary_delay_label_true)\n",
    "    print(\"Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes: \", percentage_both_conditions)\n",
    "\n",
    "    # Append overall metrics to the metrics list\n",
    "    metrics_list.append({\n",
    "        'type': 'overall',\n",
    "        'percentage_nodes_to_classify': percentage_nodes_to_classify,\n",
    "        'percentage_primary_delay_label_true': percentage_primary_delay_label_true,\n",
    "        'percentage_both_conditions': percentage_both_conditions\n",
    "    })\n",
    "\n",
    "    # Calculating metrics for freight trains\n",
    "    freight_data = data[data['freight'] == True]\n",
    "    total_nodes_freight = len(freight_data)\n",
    "    nodes_to_classify_freight = freight_data['nodes_to_classify'].sum()\n",
    "    percentage_nodes_to_classify_freight = (nodes_to_classify_freight / total_nodes_freight) * 100\n",
    "    primary_delay_label_true_freight = freight_data[freight_data['nodes_to_classify']]['primary_delay_label'].sum()\n",
    "    percentage_primary_delay_label_true_freight = (primary_delay_label_true_freight / nodes_to_classify_freight) * 100\n",
    "    relevant_nodes_freight = freight_data[freight_data['nodes_to_classify']]\n",
    "    instances_both_conditions_freight = ((relevant_nodes_freight['secondary_delay'] > 60) & (relevant_nodes_freight['primary_delay'] > 60)).sum()\n",
    "    percentage_both_conditions_freight = (instances_both_conditions_freight / len(relevant_nodes_freight)) * 100\n",
    "\n",
    "    print(\"Freight Metrics:\")\n",
    "    print(\"Percentage of nodes to classify (freight): \", percentage_nodes_to_classify_freight)\n",
    "    print(\"Percentage of nodes to classify where primary_delay_label is True (freight): \", percentage_primary_delay_label_true_freight)\n",
    "    print(\"Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes (freight): \", percentage_both_conditions_freight)\n",
    "\n",
    "    # Append freight metrics to the metrics list\n",
    "    metrics_list.append({\n",
    "        'type': 'freight',\n",
    "        'percentage_nodes_to_classify': percentage_nodes_to_classify_freight,\n",
    "        'percentage_primary_delay_label_true': percentage_primary_delay_label_true_freight,\n",
    "        'percentage_both_conditions': percentage_both_conditions_freight\n",
    "    })\n",
    "\n",
    "    # Calculating metrics for passenger trains\n",
    "    passenger_data = data[data['freight'] == False]\n",
    "    total_nodes_passenger = len(passenger_data)\n",
    "    nodes_to_classify_passenger = passenger_data['nodes_to_classify'].sum()\n",
    "    percentage_nodes_to_classify_passenger = (nodes_to_classify_passenger / total_nodes_passenger) * 100\n",
    "    primary_delay_label_true_passenger = passenger_data[passenger_data['nodes_to_classify']]['primary_delay_label'].sum()\n",
    "    percentage_primary_delay_label_true_passenger = (primary_delay_label_true_passenger / nodes_to_classify_passenger) * 100\n",
    "    relevant_nodes_passenger = passenger_data[passenger_data['nodes_to_classify']]\n",
    "    instances_both_conditions_passenger = ((relevant_nodes_passenger['secondary_delay'] > 60) & (relevant_nodes_passenger['primary_delay'] > 60)).sum()\n",
    "    percentage_both_conditions_passenger = (instances_both_conditions_passenger / len(relevant_nodes_passenger)) * 100\n",
    "\n",
    "    print(\"Passenger Metrics:\")\n",
    "    print(\"Percentage of nodes to classify (passenger): \", percentage_nodes_to_classify_passenger)\n",
    "    print(\"Percentage of nodes to classify where primary_delay_label is True (passenger): \", percentage_primary_delay_label_true_passenger)\n",
    "    print(\"Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes (passenger): \", percentage_both_conditions_passenger)\n",
    "\n",
    "    # Append passenger metrics to the metrics list\n",
    "    metrics_list.append({\n",
    "        'type': 'passenger',\n",
    "        'percentage_nodes_to_classify': percentage_nodes_to_classify_passenger,\n",
    "        'percentage_primary_delay_label_true': percentage_primary_delay_label_true_passenger,\n",
    "        'percentage_both_conditions': percentage_both_conditions_passenger\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_columns_to_integers(data):\n",
    "\n",
    "    # Check if the mapping dictionaries already exist in the current working directory\n",
    "    if os.path.exists('category_mapping.csv') and os.path.exists('operator_class_mapping.csv'):\n",
    "        # Load the existing mapping dictionaries from CSV files\n",
    "        category_mapping = pd.read_csv('category_mapping.csv', index_col='category').to_dict()['category_id']\n",
    "        operator_class_mapping = pd.read_csv('operator_class_mapping.csv', index_col='operator_class').to_dict()['operator_class_id']\n",
    "    else:\n",
    "        # Create new mapping dictionaries\n",
    "        category_mapping = {category: i for i, category in enumerate(data['category'].unique(), start=1)}\n",
    "        operator_class_mapping = {operator_class: i for i, operator_class in enumerate(data['operator_class'].unique(), start=1)}\n",
    "\n",
    "        # Export the new mapping dictionaries to CSV files in the current working directory\n",
    "        category_mapping_df = pd.DataFrame.from_dict(category_mapping, orient='index', columns=['category_id'])\n",
    "        category_mapping_df.index.name = 'category'\n",
    "        category_mapping_df.to_csv('category_mapping.csv')\n",
    "\n",
    "        operator_class_mapping_df = pd.DataFrame.from_dict(operator_class_mapping, orient='index', columns=['operator_class_id'])\n",
    "        operator_class_mapping_df.index.name = 'operator_class'\n",
    "        operator_class_mapping_df.to_csv('operator_class_mapping.csv')\n",
    "\n",
    "\n",
    "    # Map the values in the DataFrame using the mapping dictionaries\n",
    "    data['category'] = data['category'].map(category_mapping)\n",
    "    data['operator_class'] = data['operator_class'].map(operator_class_mapping)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sin_cos_time_features(data):\n",
    "\n",
    "    # Calculate the total seconds since the start of the day for both 'departure' and 'arrival'\n",
    "    data['seconds_since_midnight_departure'] = (\n",
    "        data['departure'] - data['departure'].dt.normalize()\n",
    "    ).dt.total_seconds()\n",
    "    data['seconds_since_midnight_arrival'] = (\n",
    "        data['arrival'] - data['arrival'].dt.normalize()\n",
    "    ).dt.total_seconds()\n",
    "\n",
    "    # Apply sine and cosine transformations for cyclical encoding\n",
    "    data['sin_seconds_since_midnight_departure'] = np.sin(2 * np.pi * data['seconds_since_midnight_departure'] / 86400)\n",
    "    data['cos_seconds_since_midnight_departure'] = np.cos(2 * np.pi * data['seconds_since_midnight_departure'] / 86400)\n",
    "    data['sin_seconds_since_midnight_arrival'] = np.sin(2 * np.pi * data['seconds_since_midnight_arrival'] / 86400)\n",
    "    data['cos_seconds_since_midnight_arrival'] = np.cos(2 * np.pi * data['seconds_since_midnight_arrival'] / 86400)\n",
    "\n",
    "    \n",
    "    data = data.drop(['seconds_since_midnight_departure', 'seconds_since_midnight_arrival'], axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_continuous_day_feature(data):\n",
    "    # Ensure 'arrival' is in datetime format\n",
    "    data['arrival'] = pd.to_datetime(data['arrival'])\n",
    "\n",
    "    # Extract just the date part\n",
    "    data['arrival_date'] = data['arrival'].dt.date\n",
    "\n",
    "    # Get the sorted unique dates\n",
    "    unique_days = sorted(data['arrival_date'].unique())\n",
    "\n",
    "    # Compute scaled day values\n",
    "    x_min, x_max = 0, len(unique_days) - 1  # Indices of the first and last days\n",
    "    y_min, y_max = -1, 1  # Target range for scaling\n",
    "\n",
    "    # Create a dictionary mapping each day to its scaled value\n",
    "    day_mapping = {day: y_min + ((idx - x_min) * (y_max - y_min) / (x_max - x_min)) for idx, day in enumerate(unique_days)}\n",
    "\n",
    "    # Map the dates in 'arrival_date' to their corresponding scaled value\n",
    "    data['day_value'] = data['arrival_date'].map(day_mapping)\n",
    "\n",
    "    # Drop the 'arrival_date' column if no longer needed\n",
    "    data = data.drop(['arrival_date'], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adjusts sets the arrival column to the same value as the departure column for the initial delays of freight trains\n",
    "# This is necessary, since our initial delays are sampled in the station, and in the historical data the arrival column would also already include\n",
    "# the initial delay. If we would not set this, our GNN would learn this pattern and predict worse on the historical data. \n",
    "\n",
    "def adjust_arrival_for_initial_delays_in_freight(df):\n",
    "\n",
    "    # Get unique train numbers for freight trains\n",
    "    train_numbers = df.loc[df['freight'] == True, 'train_number'].unique()\n",
    "\n",
    "    # Iterate over each freight train number\n",
    "    for train_num in train_numbers:\n",
    "        # Set arrival to departure for freight trains with train_unique_sequence 1 and ocp_type \"Stop\"\n",
    "        mask = (df['freight'] == True) & (df['train_number'] == train_num) & (df['train_unique_sequence'] == 1) & (df['ocp_type'] == 'Stop')\n",
    "        df.loc[mask, 'arrival'] = df.loc[mask, 'departure']\n",
    "        df.loc[mask, 'arrival_delay_in_seconds'] = df.loc[mask, 'departure_delay_in_seconds']\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w2/jctqt2jx2g926tqh21c367bm0000gn/T/ipykernel_57981/624504691.py:42: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Percentage of nodes to classify:  3.071153635441445\n",
      "Percentage of nodes to classify where primary_delay_label is True:  94.57935320520198\n",
      "Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes:  1.7723558522269538\n",
      "Freight Metrics:\n",
      "Percentage of nodes to classify (freight):  6.818324115621413\n",
      "Percentage of nodes to classify where primary_delay_label is True (freight):  95.86776859504133\n",
      "Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes (freight):  1.8977655341291706\n",
      "Passenger Metrics:\n",
      "Percentage of nodes to classify (passenger):  2.3071555010893245\n",
      "Percentage of nodes to classify where primary_delay_label is True (passenger):  93.80302471412763\n",
      "Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes (passenger):  1.6967908520841017\n",
      "'task_id' uniquely identifies each row.\n",
      "operator_class                          0\n",
      "train_number                            0\n",
      "ocp_type                                0\n",
      "arrival                                 0\n",
      "departure                               0\n",
      "train_unique_sequence                   0\n",
      "number_of_traction_units                0\n",
      "freight                                 0\n",
      "trainpart_weight                        0\n",
      "longitude                               0\n",
      "latitude                                0\n",
      "category                                0\n",
      "trainpart_id                            0\n",
      "departure_delay_seconds                 0\n",
      "arrival_delay_seconds                   0\n",
      "sequence_number                         0\n",
      "db640_code                              0\n",
      "sin_seconds_since_midnight_departure    0\n",
      "cos_seconds_since_midnight_departure    0\n",
      "sin_seconds_since_midnight_arrival      0\n",
      "cos_seconds_since_midnight_arrival      0\n",
      "day_value                               0\n",
      "primary_delay                           0\n",
      "task_id                                 0\n",
      "delay_jump                              0\n",
      "secondary_delay                         0\n",
      "primary_delay_label                     0\n",
      "nodes_to_classify                       0\n",
      "dtype: int64\n",
      "Transformed data exported to data/export/transformed_DelayExport_Seed_5_param_1_2_3_4_True.csv\n"
     ]
    }
   ],
   "source": [
    "def process_all_files(directory_path):\n",
    "    # Create the export directory if it does not exist\n",
    "    export_directory = os.path.join(directory_path, \"export\")\n",
    "    if not os.path.exists(export_directory):\n",
    "        os.makedirs(export_directory)\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            transformed_data = apply_transformations(file_path)\n",
    "\n",
    "            # Define the output file path\n",
    "            output_file_name = 'transformed_' + file_name\n",
    "            output_file_path = os.path.join(export_directory, output_file_name)\n",
    "\n",
    "            # Export the transformed DataFrame to a new CSV file\n",
    "            transformed_data.to_csv(output_file_path, index=False)\n",
    "            print(f\"Transformed data exported to {output_file_path}\")\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "data_directory = 'data'\n",
    "\n",
    "# Process all CSV files in the directory\n",
    "process_all_files(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Percentage of nodes to classify: \n",
      "4.065544417384062\n",
      "Average Percentage of nodes to classify where primary_delay_label is True: \n",
      "94.75004883812365\n",
      "Average Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes: \n",
      "1.788970746146742\n"
     ]
    }
   ],
   "source": [
    "# Calculate average metrics\n",
    "average_metrics = {\n",
    "    'percentage_nodes_to_classify': np.mean([metric['percentage_nodes_to_classify'] for metric in metrics_list]),\n",
    "    'percentage_primary_delay_label_true': np.mean([metric['percentage_primary_delay_label_true'] for metric in metrics_list]),\n",
    "    'percentage_both_conditions': np.mean([metric['percentage_both_conditions'] for metric in metrics_list])\n",
    "}\n",
    "\n",
    "# Print average metrics\n",
    "print(\"Average Percentage of nodes to classify: \")\n",
    "print(average_metrics['percentage_nodes_to_classify'])\n",
    "print(\"Average Percentage of nodes to classify where primary_delay_label is True: \")\n",
    "print(average_metrics['percentage_primary_delay_label_true'])\n",
    "print(\"Average Percentage of instances where both secondary_delay > 60 and primary_delay > 60 relative to relevant nodes: \")\n",
    "print(average_metrics['percentage_both_conditions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
